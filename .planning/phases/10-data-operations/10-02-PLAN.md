---
phase: 10-data-operations
plan: 02
type: execute
wave: 2
depends_on: ["10-01"]
files_modified:
  - src/GlobCRM.Infrastructure/Import/CsvParserService.cs
  - src/GlobCRM.Infrastructure/Import/ImportService.cs
  - src/GlobCRM.Infrastructure/Import/DuplicateDetector.cs
  - src/GlobCRM.Infrastructure/Import/ImportServiceExtensions.cs
  - src/GlobCRM.Infrastructure/Persistence/Repositories/ImportRepository.cs
  - src/GlobCRM.Api/Controllers/ImportsController.cs
  - src/GlobCRM.Api/Program.cs
  - src/GlobCRM.Infrastructure/GlobCRM.Infrastructure.csproj
autonomous: true

must_haves:
  truths:
    - "User can upload a CSV file and receive parsed headers with sample rows"
    - "User can submit field mapping from CSV columns to entity fields including custom fields"
    - "User can preview import with validation errors and duplicate detection before executing"
    - "Import executes in background with SignalR progress updates to the user"
    - "Import creates entities in batches with proper tenant context and permission checks"
    - "Import errors are recorded per-row with field name and error message"
  artifacts:
    - path: "src/GlobCRM.Infrastructure/Import/CsvParserService.cs"
      provides: "CSV parsing with header detection, sample rows, and streaming row-by-row reading"
      contains: "CsvParserService"
    - path: "src/GlobCRM.Infrastructure/Import/ImportService.cs"
      provides: "Core import logic: field mapping application, entity creation, batch processing, SignalR progress"
      contains: "ImportService"
    - path: "src/GlobCRM.Infrastructure/Import/DuplicateDetector.cs"
      provides: "Duplicate detection with normalized matching on email/name for contacts, name/domain for companies, title for deals"
      contains: "DuplicateDetector"
    - path: "src/GlobCRM.Api/Controllers/ImportsController.cs"
      provides: "Upload, mapping, preview, execute, status endpoints"
      contains: "ImportsController"
  key_links:
    - from: "src/GlobCRM.Api/Controllers/ImportsController.cs"
      to: "src/GlobCRM.Infrastructure/Import/ImportService.cs"
      via: "DI injection of ImportService"
      pattern: "ImportService"
    - from: "src/GlobCRM.Infrastructure/Import/ImportService.cs"
      to: "SignalR CrmHub"
      via: "IHubContext<CrmHub> for progress updates"
      pattern: "IHubContext<CrmHub>"
    - from: "src/GlobCRM.Infrastructure/Import/ImportService.cs"
      to: "IServiceScopeFactory"
      via: "Fresh DbContext per batch for memory management"
      pattern: "IServiceScopeFactory"
---

<objective>
Build the complete CSV import backend: CsvHelper-based parser, import service with batch processing and SignalR progress, duplicate detector, repository, and REST API controller with upload/map/preview/execute/status endpoints.

Purpose: Delivers the full import pipeline from CSV upload through field mapping, preview/validation, batch execution with progress tracking, to error reporting (IMPT-01 through IMPT-05).
Output: Import infrastructure services, repository, and API controller.
</objective>

<execution_context>
@/Users/metatech/.claude/get-shit-done/workflows/execute-plan.md
@/Users/metatech/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10-data-operations/10-RESEARCH.md
@.planning/phases/10-data-operations/10-01-SUMMARY.md
@src/GlobCRM.Infrastructure/CrmEntities/CrmEntityServiceExtensions.cs
@src/GlobCRM.Infrastructure/Notifications/CrmHub.cs
@src/GlobCRM.Infrastructure/Storage/IFileStorageService.cs
@src/GlobCRM.Api/Controllers/ActivitiesController.cs
@src/GlobCRM.Api/Controllers/DashboardsController.cs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install CsvHelper, create CSV parser, duplicate detector, import repository, and DI extensions</name>
  <files>
    src/GlobCRM.Infrastructure/GlobCRM.Infrastructure.csproj
    src/GlobCRM.Infrastructure/Import/CsvParserService.cs
    src/GlobCRM.Infrastructure/Import/DuplicateDetector.cs
    src/GlobCRM.Infrastructure/Persistence/Repositories/ImportRepository.cs
    src/GlobCRM.Infrastructure/Import/ImportServiceExtensions.cs
    src/GlobCRM.Api/Program.cs
  </files>
  <action>
    **Install CsvHelper:**
    ```bash
    dotnet add src/GlobCRM.Infrastructure/GlobCRM.Infrastructure.csproj package CsvHelper
    ```

    **Create CsvParserService** (Infrastructure/Import/CsvParserService.cs):
    Public methods:
    - `Task<CsvParseResult> ParseHeadersAndSampleAsync(Stream stream, int sampleSize = 100)` — reads headers + first N rows using CsvHelper dynamic record reading. Returns headers string[] and sampleRows List<Dictionary<string,string>>.
    - `IAsyncEnumerable<Dictionary<string, string>> StreamRowsAsync(Stream stream)` — yields rows one at a time for large file processing.

    Use CsvConfiguration with:
    - `HasHeaderRecord = true`
    - `MissingFieldFound = null` (don't throw)
    - `BadDataFound = null` (collect errors)
    - `TrimOptions = TrimOptions.Trim`
    - StreamReader with `Encoding.UTF8, detectEncodingFromByteOrderMarks: true`

    Define CsvParseResult record: `string[] Headers, List<Dictionary<string,string>> SampleRows, int TotalRowCount`.

    **Create DuplicateDetector** (Infrastructure/Import/DuplicateDetector.cs):
    Constructor takes ApplicationDbContext.
    Public method: `Task<List<DuplicateMatch>> DetectDuplicatesAsync(ImportEntityType entityType, List<Dictionary<string, string>> rows, List<ImportFieldMapping> mappings)`

    Duplicate matching logic per entity type:
    - **Contact:** Primary match on email (exact, case-insensitive). Secondary match on FirstName+LastName (normalized: lowercase, trimmed).
    - **Company:** Primary match on Name (normalized). Secondary match on Email/Domain.
    - **Deal:** Match on Title (normalized, case-insensitive).

    Return DuplicateMatch record: `int RowIndex, Guid ExistingEntityId, string MatchField, string MatchValue`.

    **Create ImportRepository** (Persistence/Repositories/ImportRepository.cs):
    Follow existing repository patterns (DashboardRepository). Implement IImportRepository:
    - GetByIdAsync: Include Errors navigation
    - CreateAsync: Add + SaveChanges
    - UpdateAsync: Update + SaveChanges
    - GetByUserAsync: Filter by UserId with pagination, OrderByDescending CreatedAt

    **Create ImportServiceExtensions** (Import/ImportServiceExtensions.cs):
    Follow DashboardServiceExtensions pattern:
    ```csharp
    public static IServiceCollection AddImportServices(this IServiceCollection services)
    {
        services.AddScoped<CsvParserService>();
        services.AddScoped<DuplicateDetector>();
        services.AddScoped<IImportRepository, ImportRepository>();
        // ImportService registered as transient (needs IServiceScopeFactory for batch processing)
        services.AddScoped<ImportService>();
        return services;
    }
    ```

    **Update Program.cs:** Add `builder.Services.AddImportServices();` after existing service registrations.
  </action>
  <verify>
    `dotnet build` succeeds. CsvHelper package in .csproj. CsvParserService, DuplicateDetector, ImportRepository, ImportServiceExtensions files exist.
  </verify>
  <done>
    CSV parser can read headers and stream rows. Duplicate detector checks existing entities for matches. Import repository provides CRUD. DI registered in Program.cs.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create ImportService with batch processing and ImportsController with full wizard endpoints</name>
  <files>
    src/GlobCRM.Infrastructure/Import/ImportService.cs
    src/GlobCRM.Api/Controllers/ImportsController.cs
  </files>
  <action>
    **Create ImportService** (Infrastructure/Import/ImportService.cs):
    Constructor injects: IImportRepository, CsvParserService, DuplicateDetector, IFileStorageService, IServiceScopeFactory, IHubContext<CrmHub>, ICustomFieldRepository, ILogger<ImportService>.

    Methods:

    1. `Task<ImportJob> UploadAndParseAsync(Stream fileStream, string fileName, ImportEntityType entityType, Guid userId, Guid tenantId)`:
       - Save CSV to IFileStorageService with path `imports/{tenantId}/{jobId}/{fileName}`
       - Parse headers + sample (100 rows) via CsvParserService
       - Create ImportJob with Pending status, store TotalRows from parse
       - Return the ImportJob (caller returns headers + sample to client)

    2. `Task SaveMappingAsync(Guid importJobId, List<ImportFieldMapping> mappings, string duplicateStrategy)`:
       - Load job, set Mappings = mappings, DuplicateStrategy = duplicateStrategy, Status = Mapping
       - Save

    3. `Task<ImportPreviewResult> PreviewAsync(Guid importJobId)`:
       - Load job, re-read CSV sample via IFileStorageService
       - Apply field mappings to validate: check required fields (Name for Company, FirstName+LastName for Contact, Title for Deal)
       - Validate custom fields via ICustomFieldRepository (load definitions for entity type, validate types/options)
       - Run duplicate detection on sample rows
       - Return ImportPreviewResult: validCount, invalidCount, duplicateCount, List<PreviewError> errors, List<DuplicateMatch> duplicates
       - Update job Status = Previewing

    4. `Task ExecuteAsync(Guid importJobId, Guid userId)`:
       - Load job, set Status = Processing, StartedAt = now
       - Fire-and-forget background task (Task.Run or BackgroundService dispatch):
         - Open CSV via IFileStorageService.GetFileStreamAsync
         - Stream rows via CsvParserService.StreamRowsAsync
         - Process in batches of 100 rows:
           - For each batch: create new scope via IServiceScopeFactory.CreateScope()
           - Get fresh ApplicationDbContext from scope
           - For each row: apply mappings, create entity (Company/Contact/Deal), set TenantId from job, set OwnerId = userId
           - For duplicate rows: apply strategy (skip = don't create, overwrite = update existing, merge = update only non-null fields on existing)
           - SaveChangesAsync per batch
           - Update ImportJob progress (ProcessedRows, SuccessCount, ErrorCount)
           - Send SignalR progress: `_hubContext.Clients.User(userId.ToString()).SendAsync("ImportProgress", progressDto)`
         - On completion: set Status = Completed, CompletedAt = now
         - On error: set Status = Failed, record error

    Custom field value handling: For dropdown custom fields, match CSV value against field option labels (case-insensitive). For MultiSelect, split CSV value on semicolons. For Number/Currency, parse decimal. For Date, parse ISO or common date formats. For Checkbox, parse true/false/yes/no/1/0.

    **Create ImportsController** (Api/Controllers/ImportsController.cs):
    Follow existing controller patterns (DashboardsController). Define request/response DTOs as records in the controller file.

    Endpoints:
    1. `POST /api/imports/upload?entityType={type}` — IFormFile upload, 10MB limit
       - Authorize with policy based on entityType: "Permission:{EntityType}:Create"
       - Call ImportService.UploadAndParseAsync
       - Return 201 with { importJobId, headers, sampleRows, totalRows }

    2. `POST /api/imports/{id}/mapping` — Submit field mappings
       - Body: { mappings: [{ csvColumn, entityField, isCustomField }], duplicateStrategy }
       - Call ImportService.SaveMappingAsync
       - Return 200

    3. `POST /api/imports/{id}/preview` — Dry-run validation
       - Call ImportService.PreviewAsync
       - Return 200 with { validCount, invalidCount, duplicateCount, errors, duplicates }

    4. `POST /api/imports/{id}/execute` — Start batch import
       - Call ImportService.ExecuteAsync (fire-and-forget)
       - Return 202 Accepted

    5. `GET /api/imports/{id}` — Get job status
       - Return ImportJob with status, counts, errors

    6. `GET /api/imports` — List user's import jobs
       - Paginated, ordered by CreatedAt desc
       - Return list of ImportJobDto

    Permission check: Verify the user has Create permission for the target entity type before upload. Use IPermissionService in the controller matching existing patterns.

    DTO records defined in controller:
    - UploadResponse: Guid ImportJobId, string[] Headers, List<Dictionary<string,string>> SampleRows, int TotalRows
    - MappingRequest: List<ImportFieldMapping> Mappings, string DuplicateStrategy
    - PreviewResponse: int ValidCount, int InvalidCount, int DuplicateCount, List<PreviewErrorDto> Errors, List<DuplicateMatchDto> Duplicates
    - ImportJobDto: all relevant fields from ImportJob
  </action>
  <verify>
    `dotnet build` succeeds. ImportsController exposes all 6 endpoints. ImportService handles upload, mapping, preview, and execute with batch processing. Test: `curl -X POST http://localhost:5233/api/imports/upload?entityType=Contact -F "file=@test.csv"` (with auth header) returns 201 with headers and sample.
  </verify>
  <done>
    Full import pipeline works: upload CSV -> map fields -> preview with validation/duplicate detection -> execute with batch processing and SignalR progress -> query status with errors. All endpoints require appropriate Create permission for the target entity type.
  </done>
</task>

</tasks>

<verification>
- `dotnet build` passes
- CsvHelper NuGet package installed
- POST /api/imports/upload accepts CSV file and returns headers + sample
- POST /api/imports/{id}/mapping saves field mapping
- POST /api/imports/{id}/preview returns validation results
- POST /api/imports/{id}/execute returns 202 and processes in background
- GET /api/imports/{id} returns job status with progress counts
- SignalR "ImportProgress" events sent during execution
</verification>

<success_criteria>
Complete CSV import backend is functional: file upload, header parsing, field mapping storage, preview with validation and duplicate detection, batch execution with SignalR progress, and error reporting. All operations are tenant-scoped with proper permission checks.
</success_criteria>

<output>
After completion, create `.planning/phases/10-data-operations/10-02-SUMMARY.md`
</output>
